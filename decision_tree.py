# -*- coding: utf-8 -*-
"""decision_tree.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QAscztTQiLk2ART-sd_f3yAIh-gGpZTl
"""

import pandas as pd

data = pd.read_csv("data fix.csv")
data

"""## Preprocessing Tweet "Vaksin COVID"

### Preprocessing Data
"""

# Import Library
import re
import nltk
import string
#NLTK
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords

nltk.download('stopwords')

"""#### 1. Remove Regex (Cleansing)"""

# Menghilangkan kalimat Encode
data['Komen'].replace(to_replace = r'\\x[0-9a-fA-F][0-9a-fA-F]', value = '', regex = True, inplace = True)
data
def hello (Komen):
    print("hello world")

def remove(Komen):
    # remove stock market tickers like $GE
    Komen = re.sub(r'\$\w*', '',str(Komen ))
    # Remove RT/b/ yang tersisa
    Komen = re.sub(r'\bRT\b', '', Komen)
    Komen  = re.sub('b\'', '', Komen)    
    # Replace 2+ dots with space
    Komen = re.sub(r'\.{2,}', ' ', Komen)
    #remove @username
    Komen = re.sub('@[^\s]+','',Komen)
     # remove old style retweet text "RT"
    Komen = re.sub(r'^RT[\s]+', '', Komen)
    #remove angka
    Komen = re.sub('[0-9]+', '', Komen)
    #remove url
    Komen = re.sub(r"http\S+", "", Komen)
    # remove hashtags
    Komen = re.sub(r'#\w*', '', Komen)
    # Strip space, " and ' from tweet
    Komen = Komen.strip(' "\'')
    # Replace multiple spaces with a single space
    Komen = re.sub(r'\s+', ' ', Komen)
    #hapus tanda baca
    Komen = Komen.translate(str.maketrans("","",string.punctuation))
    #hapus karakter
    Komen = re.sub(r'\n', '', Komen)

    return Komen 
data['clean'] = data['Komen'].apply(lambda x: remove(x))
 
data

"""#### 2. Case Folding"""

# proses case folding 
data['case_folding'] = data['clean'].str.lower()
data

"""#### 3. Slang Word"""

# Import Kamus
df_slang = pd.read_csv('colloquial-indonesian-lexicon.csv')

# Proses Slang Word
def replace_slang(tweets):
    tweets = tweets.lower()
    res = ''
    for item in tweets.split():
        if item in df_slang.slang.values:
            res += df_slang[df_slang['slang'] == item]['formal'].iloc[0]
        else:
            res += item
        res += ' '
    return res

data['slang_word'] = data['case_folding'].apply(replace_slang)
data

"""#### 4. Tokenizing"""

# proses tokenizing, proses pemisahan kata
from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt')
#NLTK word tokenize
def word_tokenize_wrapper(Tweets):
 return word_tokenize(Tweets)
data['Tokenizing'] = data['slang_word'].apply(word_tokenize_wrapper)
data

"""#### 5. Filtering (Stopword Removal)"""

#Proses Stopword Removal
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
def stopword_removal(Tweets):
    filtering = stopwords.words('indonesian')
    x = []
    data = []
    def myFunc(x):
        if x in filtering:
            return False
        else:
            return True
    fit = filter(myFunc, Tweets)
    for x in fit:
        data.append(x)
    return data
data['Stopword'] = data['Tokenizing'].apply(stopword_removal)
data

"""#### 6. Stemming"""

# proses stemming

from sklearn.pipeline import Pipeline
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

def stemming(Tweets):
    factory = StemmerFactory()
    stemmer = factory.create_stemmer()
    do = []
    for w in Tweets:
        dt = stemmer.stem(w)
        do.append(dt)
    d_clean=[]
    d_clean=" ".join(do)
    print(d_clean)
    return d_clean
data['Hasil'] = data['Stopword'].apply(stemming)
data

data.to_csv('afterpreprocessing.csv', index=False)

# Seleksi Data
data_final = data.loc[:, ['Date', 'Username', 'Komen','Label', 'clean', 'case_folding', 'Tokenizing', 'Stopword', 'Hasil']]

data_final

data_selection = data.loc[:, ['Hasil', 'Label']]
data_selection

data_final.to_csv('hasilpreprocessing.csv')

data_selection.to_excel("datafixstemming.xlsx")

data = pd.read_excel("datafixstemming.xlsx")
data

data.duplicated()

data_clean = data.drop_duplicates()
data_clean

"""Kategorikan Data"""

data_drop_cleaning= pd.read_excel('datafixstemming.xlsx')
data_drop_cleaning

data_drop_cleaning = data_drop_cleaning.astype({'Label' : 'category'})
data_drop_cleaning = data_drop_cleaning.astype({'Hasil' : 'string'})
data_drop_cleaning.dtypes

"""### Proses Pembobotan Teks Menggunaka Algoritma TF-IDF"""

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

#Perhitungan TF
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(data_drop_cleaning['Hasil'].astype('U'))
#Perhitungan TF-IDF
tf = TfidfVectorizer()
text_tf = tf.fit_transform(data_drop_cleaning['Hasil'].astype('U'))
print(text_tf)

"""Melihat Hasil Letak Sebuah Kata"""

vectorizer.get_feature_names()

"""Melihat hasil kalkulasi dari TF-IDF dalam sebuah kata"""

text_tf.todense()

df = pd.DataFrame(text_tf.todense().T, 
                  index=vectorizer.get_feature_names(),
                  columns=[f'D{i+1}' for i in range(len(data_drop_cleaning))])
df.head(15)

df.to_excel("hasil tfidff.xlsx")

"""### Spliting Data
Untuk Data Training 0.8 dan Data Test 0.2
"""

# splitting data 
import collections, numpy
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(text_tf, data_drop_cleaning['Label'], test_size=0.2, random_state=0)
pos = (y_test == 'P').sum()
neg = (y_test == 'N').sum()
postrain = (y_train == 'P').sum()
negtrain = (y_train == 'N').sum()
total = pos + neg 
print("Jumlah data uji dengan sentimen positif:", pos)
print("Jumlah data uji dengan sentimen negatif:",neg)
print("Jumlah data latih dengan sentimen positif:", postrain)
print("Jumlah data latih dengan sentimen negatif:",negtrain)
data_drop_cleaning['Label'].value_counts()

print(X_test)

"""### Perform Algoritma Decision Tree"""

from sklearn.tree import DecisionTreeClassifier
from sklearn import metrics

classifier = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=1)
classifier.fit(X_train, y_train)
 
 
predictions = classifier.predict(X_test)
 
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

print("The Confusion Matrix is:\n")
print(confusion_matrix(y_test,predictions))  
print("\n The Classification Report is:\n")
print(classification_report(y_test,predictions))
print("\n The accuracy Score is:\n")  
print(accuracy_score(y_test, predictions))

"""### Pengujian terhadap kebenaran teks"""

# Input kalimat teks
kalimat_tes = ["vaksin booster menyehatkan"]

# definisikan nama label
nama_label = ["P","N"]

#Loop untuk prediksi
for teks in kalimat_tes:
    arr_teks = []
    arr_teks.append(teks)
    vektor = vectorizer.transform(arr_teks)
    prediksi_label_tree = classifier.predict(vektor)

prediksi_label_tree

"""## Word Cloud"""


import matplotlib.pyplot as plt
from wordcloud import WordCloud, ImageColorGenerator
from wordcloud import WordCloud
import matplotlib.pyplot as plt

"""### Word Cloud Teks Positif"""

import pandas as pd
#Word Cloud Teks Positif
f = open("Teks Positif.txt", "r")

isi_text_positif = f.read()

#baca data
print(isi_text_positif)

wordcloud = WordCloud(width=1600, height=800, max_font_size=200, max_words=80, colormap='Set2', background_color='black')

wordcloud.generate(isi_text_positif)

plt.figure(figsize=(12,10))

plt.imshow(wordcloud, interpolation='bilinear')
plt.savefig('wordcloud_positif.png')
plt.axis("off")

plt.show()

#Word Freq for Positive text

# create a dictionary of word frequencies
text_dictionary = wordcloud.process_text(isi_text_positif)
# sort the dictionary
word_freq_pos = []
rel_freq_pos = []

word_freq_pos = {k: v for k, v in sorted(text_dictionary.items(),reverse=True, key=lambda item: item[1])}

#use words_ to print relative word frequencies
rel_freq_pos = wordcloud.words_

#print results
print(list(word_freq_pos.items())[:10])
print(list(rel_freq_pos.items())[:10])

import numpy as np
wfp_all = list(word_freq_pos.items())[:10]
word_freq_pos_word = np.array(wfp_all)
word_freq_pos_word_2 = np.flip(wfp_all, axis=None)

print(word_freq_pos_word_2[:,0])
print(word_freq_pos_word_2[:,1])

# Pass the x and y cordinates of the bars to the
# function. The label argument gives a label to the data.
plt.bar(word_freq_pos_word_2[:,1],word_freq_pos_word_2[:,0], label="Possitive Words", color = 'green')
plt.legend()

# changing the rc parameters and plotting a line plot
plt.rcParams['figure.figsize'] = [20, 10]

# The following commands add labels to our figure.
plt.xlabel('Terms')
plt.ylabel('Frequency of Terms')
plt.title('Word Frequency of Positive Text')
plt.savefig('grafik_positif.png')

plt.show()

"""### Word Cloud Teks Negatif"""

#Word Cloud Teks Negatif
f = open("Teks Negatif.txt", "r")

isi_text_negatif = f.read()

print(isi_text_negatif)

wordcloud = WordCloud(width=1600, height=800, max_font_size=200, max_words=80, colormap='Set2', background_color='black')

wordcloud.generate(isi_text_negatif)

plt.figure(figsize=(12,10))

plt.imshow(wordcloud, interpolation='bilinear')
plt.savefig('wordcloud_negatif.png')
plt.axis("off")

plt.show()

#Word Freq for Negative text

# create a dictionary of word frequencies
text_dictionary = wordcloud.process_text(isi_text_negatif)
# sort the dictionary
word_freq_neg = []
rel_freq_neg = []

word_freq_neg = {k: v for k, v in sorted(text_dictionary.items(),reverse=True, key=lambda item: item[1])}

#use words_ to print relative word frequencies
rel_freq_neg = wordcloud.words_

#print results
print(list(word_freq_neg.items())[:10])
print(list(rel_freq_neg.items())[:10])

import numpy as np
wfn_all = list(word_freq_neg.items())[:10]
word_freq_neg_word = np.array(wfn_all)
word_freq_neg_word_2 = np.flip(wfn_all, axis=None)

print(word_freq_neg_word_2[:,0])
print(word_freq_neg_word_2[:,1])

# Pass the x and y cordinates of the bars to the
# function. The label argument gives a label to the data.
plt.bar(word_freq_neg_word_2[:,1],word_freq_neg_word_2[:,0], label="Negative Word", color = 'red')
plt.legend()

# changing the rc parameters and plotting a line plot
plt.rcParams['figure.figsize'] = [20, 10]



# The following commands add labels to our figure.
plt.xlabel('Terms')
plt.ylabel('Frequency of Terms')
plt.title('Word Frequency of Negative Text')
plt.savefig('grafik_negatif.png')

plt.show()



